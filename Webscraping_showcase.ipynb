{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uuATYYtzkB2"
      },
      "source": [
        "**Tool Showcase**\n",
        "=======\n",
        "\n",
        "<font color = 'E3A440'>*Webscraping tutorial*</font>\n",
        "=============\n",
        "\n",
        "This tutorial is a short hands-on tutorial to introduce the webscraping pratice.\n",
        "It was presented during the <font color = 'E3A440'>Tool Showcase</font> at [P4IE Conference - Measuring Metrics that Matter](https://event.fourwaves.com/p4ie/pages), which took place on 9-10-11 May 2022 at the *Hilton Garden Inn*, in Ottawa.\n",
        "\n",
        "Structure of the showcase:\n",
        "1. General Framework\n",
        "2. Web scraping step by step\n",
        "3. Launch a program \n",
        "\n",
        "This tutorial cannot be considered as ehaustif of the domain. We use simple steps and basic functions for demonstrative purpose.\n",
        "\n",
        "### Author: \n",
        "- Davide Pulizzotto <davide.pulizzotto@polymtl.ca>\n",
        "\n",
        "### Table of Contents\n",
        "\n",
        "- [Section 1. Introduction](#introduction)\n",
        "- [Section 2. Step by step](#step-by-step)\n",
        "- [Section 3. Launch a program](#Launch)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "232GZfpEzkB5"
      },
      "source": [
        "<a id='introduction'></a>\n",
        "# <font size = '6' color='E3A440'>Section 1. Introduction</font>\n",
        "\n",
        "This tool showcase is based on some basic functions and packages of **Python**. During the showcase, we implement a program which scrape a journal from Taylor & Francis Editors, which is the [Journal of Responsible Innovation](https://www.tandfonline.com/action/journalInformation?show=aimsScope&journalCode=tjri20). This editor was chosen after having analysed the `robots.txt` file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is web scraping?\n",
        "\n",
        "![WebHavry](https://www.webharvy.com/images/web%20scraping.png)"
      ],
      "metadata": {
        "id": "bgpDo_3CAx6p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Web as source of information"
      ],
      "metadata": {
        "id": "8ppTC7vUC6Py"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "HTML Structure:\n",
        "\n",
        "![stackoverflow](https://i.stack.imgur.com/dmICu.png)\n",
        "\n",
        "HTML document:\n",
        "\n",
        "![Devpost](https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/001/573/508/datas/original.png)\n",
        "\n",
        "HTML tag element:\n",
        "\n",
        "![Wikipedia](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "L--i9yFbCaIQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Python for web scraping\n",
        "\n",
        "The web scraping software is a program wrote in some language. This showcase is based on some of the simplest modules and functions existing in **Python**. In short, we will use two packages:\n",
        "\n",
        "1. `requests`: to make requests on the web\n",
        "2. `BeautifulSoup`: to parse HTML document and get information"
      ],
      "metadata": {
        "id": "btF-eEhsD4lK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Data structure\n",
        "\n",
        "In this showcase, we will use a *tabular format* to store data coming from web scraping. This format is easy and familiar. Nonetheless, i's not the best format to store data from web scraping. We invite you to explore docuemnt-based data steructuere, which use *json format* , sucvh as **MongoDB**.\n",
        "\n",
        "A tabular format is a simple table like that.\n",
        "\n",
        "![Microsoft](https://support.content.office.net/en-us/media/3dd2b79b-9160-403d-9967-af893d17b580.png)\n",
        "\n",
        "In this showcase, we will use **dataframe** managed by `pandas`."
      ],
      "metadata": {
        "id": "6YXg-y1IEx3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Web scraping with respect\n",
        "\n",
        "In order to do not fall in illegal pratices, you do not have to perceive the web scraping as a world where everythig is permitted. \n",
        "\n",
        "When you begin a web scraping project, be sure to respect the indication in the `robots.txt` file contained in each website. \n",
        "\n",
        "[Here](https://www.tandfonline.com/robots.txt) our `robots.txt`file"
      ],
      "metadata": {
        "id": "M41HTBbrGrxd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='step-by-step'></a>\n",
        "# <font size = '6' color='E3A440'>Section 2. Step by step</font>\n",
        "\n"
      ],
      "metadata": {
        "id": "t31GHSSxdi2G"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRKQ8i7BzkB7"
      },
      "source": [
        "## 2.1 Import packges\n",
        "\n",
        "In order to work correctly, we need to prepare the environment by imeximport necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import time\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "x7LcqedRb3zI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prapare the headers to use."
      ],
      "metadata": {
        "id": "Ecysb30-Gdds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}"
      ],
      "metadata": {
        "id": "DskUc3Maf73R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Make the request to the server\n",
        "\n",
        "To make a request, we need a url, such [this](https://www.tandfonline.com/loi/tjri20).\n",
        "\n",
        "Get the url of an issue of the journal and use `requests` to complete the task."
      ],
      "metadata": {
        "id": "kEIc_XnuDOuL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response_req = requests.get('',\n",
        "                            headers = headers,\n",
        "                            timeout = 10)"
      ],
      "metadata": {
        "id": "-GnvHP0cEnic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which is the response from the server? \n",
        "\n",
        "Give a look to the [HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)"
      ],
      "metadata": {
        "id": "B1G3ZwW9H9Tj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response_req.status_code"
      ],
      "metadata": {
        "id": "ccLwGNv-Tl7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Parse the response's content"
      ],
      "metadata": {
        "id": "qAzfHRB4DSRy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "IF you have a `200` status code, you can now parse the HTML document with `BeautifulSoup`."
      ],
      "metadata": {
        "id": "P924znHzIbBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if response_req.status_code == 200:\n",
        "    soup = BeautifulSoup(response_req.content, features=\"lxml\")"
      ],
      "metadata": {
        "id": "ZQjmkXqwgPpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Go into action\n",
        "\n",
        "You can now begin to exrtact information. \n",
        "\n",
        "For exemple, get the title of the volume. \n",
        "\n",
        "How can we know how to get twe tag of the informaiton we want? Give a look at the HTML source of your web page to scrape! \n",
        "\n",
        "Check [here](https://www.tandfonline.com/toc/tjri20/9/1?nav=tocList)"
      ],
      "metadata": {
        "id": "PjyhYq4ueFSV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3.1 Get information by tag\n",
        "\n",
        "The method `.find()` looks at the `name` of the **tag** and a dictionary of **attributes** with theirs {`name`:`value`}\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LehkYf97m2OG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "title_issue_el = soup.find(name = \"div\", attrs={'class' : \"toc-title\"})\n",
        "title_issue_el.text"
      ],
      "metadata": {
        "id": "MVXRgPnyJbTF",
        "outputId": "41577867-03a9-42a2-ad2d-d4edb8e5b584",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Journal of Responsible Innovation, Volume 9, Issue 1 (2022)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now extract a part of the title if we only need specific information. For this kind of operation, you have to know regular expressions (REGEX). We use `re` package for that kind of operation."
      ],
      "metadata": {
        "id": "qFk3ZCfUKR0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Year\n",
        "title_issue_el = soup.find(\"div\", \"toc-title\")\n",
        "year = re.search(\"(\\()([0-9]{4})(\\))\", title_issue_el.text).groups()[1]\n",
        "year"
      ],
      "metadata": {
        "id": "6A4bRdARiqnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3.2 Get all occurences of the same tag\n",
        "\n",
        "We use the method `findAll` to get all the occurrences of the same tag. For exmeple, we need all occurences of the tag containing informaiton about articles of the issue. \n",
        "\n",
        "Give a look to the HTML [here](https://www.tandfonline.com/toc/tjri20/9/1?nav=tocList)"
      ],
      "metadata": {
        "id": "-tEi9HEKhyow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_articles = soup.findAll('div', attrs={\"class\": \"art_title linkable\"})"
      ],
      "metadata": {
        "id": "SohAjms2g_3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(list_articles)"
      ],
      "metadata": {
        "id": "_nSXpcmXMg7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print content of these tags"
      ],
      "metadata": {
        "id": "olj1uCmdGAut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for x in list_articles:\n",
        "    print(x.text)"
      ],
      "metadata": {
        "id": "q8K9jK1QhTm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also can extract informaiton from tag's attributes, such as the url of an article."
      ],
      "metadata": {
        "id": "Pmk12Tk-eiN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_url_articles = []\n",
        "for art_element in list_articles:\n",
        "    for link_ in art_element.findAll(\"a\", href=True):\n",
        "        print(link_)\n",
        "        list_url_articles.append(link_['href'])\n"
      ],
      "metadata": {
        "id": "As-K9yOeTfHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add prefix to the urls."
      ],
      "metadata": {
        "id": "TgTIU6sJNVEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for url_article in list_url_articles:\n",
        "    url_art_temp = f\"https://www.tandfonline.com{url_article}\"\n",
        "    print(url_art_temp)"
      ],
      "metadata": {
        "id": "w9y-EHkNl7G_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3.3 Enter in the article a web page\n",
        "\n",
        "Using that list of url, we can get new information and generalize the code to several elements."
      ],
      "metadata": {
        "id": "XyV1Fo0wkpHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url_article = list_url_articles[0]\n",
        "url_article = f\"https://www.tandfonline.com{url_article}\"\n",
        "print(url_article)"
      ],
      "metadata": {
        "id": "ODphaMbNn6is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_req_article = requests.get(url_article,\n",
        "                        headers = headers,\n",
        "                        timeout = 10)\n",
        "if response_req_article.status_code == 200:\n",
        "    soup_article = BeautifulSoup(response_req_article.content, features=\"lxml\")"
      ],
      "metadata": {
        "id": "2U8zeL3YGrxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get information about the type of the article"
      ],
      "metadata": {
        "id": "CaTlfMG2pmhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "type_article = soup_article.find(name = \"div\", attrs = {\"class\": \"toc-heading\"}).text\n",
        "print(type_article)"
      ],
      "metadata": {
        "id": "ZD9hNHXVYJvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the title of the article"
      ],
      "metadata": {
        "id": "p0Kz_3c7pp_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "title = soup_article.find(name = \"span\", attrs = {\"class\": re.compile(\"article-title\")})\n",
        "print(title.text)"
      ],
      "metadata": {
        "id": "Gs0QT8wGHJUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the list of authors"
      ],
      "metadata": {
        "id": "_sMblqhVpsSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_authors = []\n",
        "for author_temp in soup_article.findAll(name = \"a\", attrs = {'class': \"author\"}):\n",
        "    list_authors.append(author_temp.text)\n",
        "    print(author_temp.text)"
      ],
      "metadata": {
        "id": "itvZAe0-J8-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "authors = '; '.join(list_authors)\n",
        "authors"
      ],
      "metadata": {
        "id": "WCVgCSBBL9s-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the abstract content"
      ],
      "metadata": {
        "id": "sa2JhqDmMaNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "abstract_el =  soup_article.find(name = \"div\", attrs = {'class': \"abstractSection abstractInFull\"})\n",
        "abstract_text = abstract_el.find(name = 'p').next_sibling.text\n",
        "abstract_text"
      ],
      "metadata": {
        "id": "VCmQqg4jMHLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's time to prepare your programe and to launch it! "
      ],
      "metadata": {
        "id": "3aYS5nM2rsmV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='Launch'></a>\n",
        "# <font size = '6' color='E3A440'>Section 3. Launch a program</font>\n"
      ],
      "metadata": {
        "id": "NAxpB8pWp0lC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Prepare data structure"
      ],
      "metadata": {
        "id": "Vydy-rDQrvs9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_data_jri = pd.DataFrame(columns=['Vol','Issue','Year','Authors', 'Title','Abstract','url'])"
      ],
      "metadata": {
        "id": "rAyoF3Z3mrPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_data_jri"
      ],
      "metadata": {
        "id": "8-WL67tOQkPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Launch your web scraping program"
      ],
      "metadata": {
        "id": "uxCHb5sVrzpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Volumes\n",
        "for vol_ in [7, 8, 9]:\n",
        "    # Issues\n",
        "    for issue_ in [1,2,3]:\n",
        "        url_issue = f\"https://www.tandfonline.com/toc/tjri20/{vol_}/{issue_}?nav=tocList\"\n",
        "        issue_req = requests.get(url_issue,\n",
        "                                    headers = headers,\n",
        "                                    timeout = 10)\n",
        "        if issue_req.status_code == 200:\n",
        "            soup_issue = BeautifulSoup(issue_req.content, features=\"lxml\")\n",
        "        \n",
        "        # Year\n",
        "        title_issue_el = soup_issue.find(\"div\", \"toc-title\")\n",
        "        year = re.search(\"(\\()([0-9]{4})(\\))\", title_issue_el.text).groups()[1]\n",
        "        print(f\"\\n{title_issue_el.text}\\n\")\n",
        "        # Articles\n",
        "        list_url_articles = soup_issue.findAll('div', attrs={\"class\":re.compile(\"art_title linkable\")})\n",
        "        for url_article in list_url_articles:\n",
        "            time.sleep(1)\n",
        "            url_arti_href  = url_article.find(\"a\")['href']\n",
        "            url_art_temp = f\"https://www.tandfonline.com{url_arti_href}\"\n",
        "            req_article = requests.get(url_art_temp,\n",
        "                                    headers = headers,\n",
        "                                    timeout = 10)\n",
        "            if req_article.status_code == 200:\n",
        "                soup_article = BeautifulSoup(req_article.content, features=\"lxml\")\n",
        "                # print(soup_article.find(\"span\", attrs = {\"class\": \"article-type\"}).text)\n",
        "                \n",
        "                if not re.search(\"Article\", soup_article.find(\"div\", attrs = {\"class\": \"toc-heading\"}).text):\n",
        "                    continue\n",
        "            # title\n",
        "            title = soup_article.find(\"span\", attrs = {\"class\": re.compile(\"article-title\")})\n",
        "            print(title.text)\n",
        "            # author\n",
        "            list_authors = []\n",
        "            for author_temp in soup_article.findAll(\"a\", attrs = {'class': \"author\"}):\n",
        "                list_authors.append(author_temp.text)\n",
        "            authors = '; '.join(list_authors)\n",
        "            # abstract\n",
        "            abstract_el =  soup_article.find(\"div\", attrs = {'class': \"abstractSection abstractInFull\"})\n",
        "            abs_list_par = []\n",
        "            for par_ in abstract_el.findAll('p'):\n",
        "                if not re.search(\"^abstract\", par_.text, re.IGNORECASE):\n",
        "                    abs_list_par.append(par_.text)\n",
        "            abstract_text = '\\n'.join(abs_list_par)\n",
        "            # fill database\n",
        "            idx = len(df_data_jri)\n",
        "            df_data_jri.loc[idx] = [vol_, issue_, year, authors, title.text, abstract_text, url_art_temp]"
      ],
      "metadata": {
        "id": "7vEN6Xmlf3PK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_data_jri"
      ],
      "metadata": {
        "id": "Rpz1Y6NeovfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Analyze your data\n",
        "\n",
        "We now can analyze data extracted from web pages. \n",
        "\n",
        "To make an exemple, we count most frequent authors and words in abstracts."
      ],
      "metadata": {
        "id": "TFGIfPPbr7xt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get most frequent authors. In particular, we extract authors having more then 1 signature."
      ],
      "metadata": {
        "id": "EhbwvrWssBWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "counter_authors = collections.Counter([aut_.strip() for aut_string in df_data_jri['Authors'] for aut_ in aut_string.split(\"; \")])\n",
        "[x for x in counter_authors.items() if x[1] > 1]"
      ],
      "metadata": {
        "id": "7o78Fbi88nzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get most frequent words in textual fields."
      ],
      "metadata": {
        "id": "EHPN97fisFne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next chunck of code is necessary to execute analysis."
      ],
      "metadata": {
        "id": "S8CLKYhU79a1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class P4IE_webscraping_showcase:\n",
        "    def __init__(self, data_df, textual_column = \"\"):\n",
        "        !pip install nltk\n",
        "        info_modules_spacy = !python -m spacy validate\n",
        "        download_model = True\n",
        "        for x in info_modules_spacy:\n",
        "            if re.search(\"en_core_web_sm\", x):\n",
        "                download_model = False\n",
        "        if download_model:\n",
        "            !python -m spacy download en_core_web_sm\n",
        "        from nltk.corpus import stopwords\n",
        "        from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "        from wordcloud import WordCloud\n",
        "        import spacy\n",
        "        import nltk\n",
        "        import tqdm\n",
        "        #\n",
        "        self.nlp = spacy.load('en_core_web_sm')\n",
        "        self.data_df = data_df\n",
        "        print(\"Executing data preprocess..\")\n",
        "        self.data_df[\"text_preprocessed\"] = list(self.nlp.pipe(data_df[textual_column], disable = [\"tok2vec\",'parser','ner']))\n",
        "        #\n",
        "        # download default list of stopword\n",
        "        nltk.download('stopwords')\n",
        "        # add a custom word to the stopword list\n",
        "        stopwords_list = set(stopwords.words('english') + ['would'])\n",
        "        # initialize empty lists\n",
        "        text_cleaned = []\n",
        "        # iterate over each preprocessed document\n",
        "        print(\"Removing irrelevant tokens...\")\n",
        "        for idx, row in self.data_df.iterrows(): \n",
        "            # keep only the lemma for each token that has been tagged as one of these POS tags [\"ADV\", \"ADJ\", \"NOUN\", \"VERB\"] AND its lemma IS NOT contained in the stopwords_list AND its lemma has more than 1 character\n",
        "            text = [w.lemma_.lower() for w in row['text_preprocessed'] if w.pos_ in [\"ADV\", \"ADJ\", \"NOUN\", \"VERB\"] and w.lemma_.lower() not in stopwords_list and len(w.lemma_.lower())> 1]\n",
        "            text_cleaned.append(text)\n",
        "        self.data_df[\"text_preprocessed\"] = text_cleaned\n",
        "        #\n",
        "        # vectorization\n",
        "        def identity_tokenizer(text):\n",
        "            return text\n",
        "\n",
        "        # Transforming the word in frequencies\n",
        "        self.vectorized = CountVectorizer(lowercase = False, # Convert all characters to lowercase before tokenizing\n",
        "                                    min_df = 2, # Ignore terms that have a document frequency strictly lower than the given threshold \n",
        "                                    max_df = 50, # Ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words)\n",
        "                                    stop_words = stopwords_list, # Remove the list of words provided\n",
        "                                    ngram_range = (1, 1), # Get the lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted\n",
        "                                    tokenizer=identity_tokenizer) # Override the string tokenization step while preserving the preprocessing and n-grams generation steps\n",
        "        #\n",
        "        self.freq_term_DTM = self.vectorized.fit_transform(self.data_df[\"text_preprocessed\"])\n",
        "        #\n",
        "        # Calculate the tfidf matrix\n",
        "        tfidf = TfidfTransformer(norm='l1')\n",
        "        self.tfidf_DTM = tfidf.fit_transform(self.freq_term_DTM)\n",
        "        #\n",
        "        print(\"Data are now ready to be analyzed!\")\n",
        "\n",
        "\n",
        "    def prepare_data_for_WC(self, DTM, vocabulary_dtm):\n",
        "        import numpy as np\n",
        "        import scipy\n",
        "        # compute total frequency for each word\n",
        "        values_words = sum(DTM)\n",
        "        # values_words = sum(tfidf_matrix)\n",
        "        # verify type result and prepare data for wordcloud\n",
        "        if type(values_words) is np.ndarray:\n",
        "            values_words = [float(value) for value in np.nditer(values_words)]\n",
        "        elif type(values_words) is scipy.sparse.csr.csr_matrix:\n",
        "            values_words = [float(value) for value in np.nditer(values_words.todense())]\n",
        "        else:\n",
        "            print(\"Matrix in argument DTM has to be one of these two data classes:  'scipy.sparse.csr.csr_matrix' or 'numpy.ndarray'\")\n",
        "        ##Retrieve the word fromthe vocaboulary and sorting them based on the frequency\n",
        "        list_mots = sorted(vocabulary_dtm.items(), key= lambda x:x[1])\n",
        "        list_mots = [word for (word,idx) in  list_mots]\n",
        "        words = zip(list_mots, values_words)\n",
        "        words = sorted(words, key= lambda x:x[1], reverse=True)\n",
        "        ## prepare data structure for wordcloud\n",
        "        result_for_WC = {}\n",
        "        #iterating over the tuples lists\n",
        "        for (key, value) in words:\n",
        "            if value == 0:\n",
        "                result_for_WC[key] = 0.00000000000001\n",
        "            else:\n",
        "                result_for_WC[key] = value\n",
        "        #\n",
        "        return result_for_WC\n",
        "    \n",
        "    def show_wordcloud(self, result_for_WC, title_in_plot = \"Most frequent words\"):\n",
        "        import matplotlib.pyplot as plt\n",
        "        from wordcloud import WordCloud\n",
        "        plot = WordCloud().generate_from_frequencies(result_for_WC)\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        plt.imshow(plot)\n",
        "        plt.title(title_in_plot,\n",
        "                fontsize = 32,\n",
        "                bbox=dict(facecolor='red', alpha=0.5))\n",
        "        plt.axis('off')\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "vHYg5B0KsXzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We initialize aour `analyzer` executing a first preprocessing of data."
      ],
      "metadata": {
        "id": "Z55Npird8FkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Analyzer_data = P4IE_webscraping_showcase(df_data_jri,textual_column = \"Abstract\")"
      ],
      "metadata": {
        "id": "rNE637i8tJuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We prepare data for a `wordcloud`."
      ],
      "metadata": {
        "id": "-MrroYxJ8M23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_for_WC = Analyzer_data.prepare_data_for_WC(Analyzer_data.freq_term_DTM, Analyzer_data.vectorized.vocabulary_)"
      ],
      "metadata": {
        "id": "tSN5RnRzti1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_for_WC"
      ],
      "metadata": {
        "id": "jN8rWPXu62td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We show the wordcloud"
      ],
      "metadata": {
        "id": "ITEjhreI8UD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Analyzer_data.show_wordcloud(data_for_WC)"
      ],
      "metadata": {
        "id": "xJFNHqU82BJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get most frequent words by Volume"
      ],
      "metadata": {
        "id": "-0ZJqXG0sJLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Nvol = 7\n",
        "selected_rows = np.array(df_data_jri['Vol'] == Nvol)\n",
        "data_for_WC = Analyzer_data.prepare_data_for_WC(Analyzer_data.tfidf_DTM[selected_rows], Analyzer_data.vectorized.vocabulary_)\n",
        "Analyzer_data.show_wordcloud(data_for_WC, title_in_plot = f\"Most frequent words of Vol. {Nvol}\")"
      ],
      "metadata": {
        "id": "WoBw8eZ43sT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Nvol = 8\n",
        "selected_rows = np.array(df_data_jri['Vol'] == Nvol)\n",
        "data_for_WC = Analyzer_data.prepare_data_for_WC(Analyzer_data.tfidf_DTM[selected_rows], Analyzer_data.vectorized.vocabulary_)\n",
        "Analyzer_data.show_wordcloud(data_for_WC, title_in_plot = f\"Most frequent words of Vol. {Nvol}\")"
      ],
      "metadata": {
        "id": "FMhjfeSzbSRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Nvol = 9\n",
        "selected_rows = np.array(df_data_jri['Vol'] == Nvol)\n",
        "data_for_WC = Analyzer_data.prepare_data_for_WC(Analyzer_data.tfidf_DTM[selected_rows], Analyzer_data.vectorized.vocabulary_)\n",
        "Analyzer_data.show_wordcloud(data_for_WC, title_in_plot = f\"Most frequent words of Vol. {Nvol}\")"
      ],
      "metadata": {
        "id": "TPzmpI_pbhoG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Webscraping_showcase.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}